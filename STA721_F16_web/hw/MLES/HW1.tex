\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{amsmath,amssymb,array,comment,eucal}
\pagestyle{empty}
\input{../../../Lectures/macros}
\begin{document}
{\bf STA721 \hfill Homework 1}

\vspace{.5in}
\noindent
Work the following problems from Christensen (C) and Wakefield (W)
\begin{enumerate}
\item 5.8 (W)  (see link to eBook on Calendar)
\item 1.5.8 (C) (see link to eBook on Calendar)

\item We showed that $\P_\X = \X(\X^T\X)^{-1}\X^T$ was an orthogonal
  projection on the column space of $\X$ and that $\hat{\Y} = P_\X \Y$.
  While useful for theory, the projection matrix should never be used
  in practice to find the MLE of $\mub$ due to 1) computational
  complexity (inverses and matrix multiplication) and instability.  To
  find $\hat{\b}$ we solve $\X \b = \P_\X \Y $ which
  leads to the {\it normal equations}  $(\X^T\X) \b = \X^T\Y$ and
  solving the system of equations for $\b$.
  Instead consider the following for $\X$ ($n \times p, p < n$) of rank $p$

  \begin{enumerate}
  \item Any $\X$ may be written via a singular value decomposition as
    $\U \Lambdab \V^T$ where $\U$ is a $n \times p$ orthonormal matrix
    ($\U^T\U = \I_p$ and columns of $\U$ form an orthonormal basis (ONB) for
    $C(\X)$), $\Lambdab$ is a $p \times p$ diagonal matrix and $\V$ is
    a $p \times p$ orthogonal matrix ($\V^T\V = \V \V^T = \I_p$. Note
    the difference between {\it orthonormal} and {\it orthogonal}.
    Show that $\P_X$ may be expressed as a function of $\U$ only and
    provide an expression for $\hat{\Y}$.  Similarly, find an
    expression for $\hat{\b}$ in terms of $\U$, $\Lambdab$ and $\V$.
    Your result should only require the inverse of a diagonal matrix!
\item $\X$ may be written in a (reduced or thinned) QR decomposition as a matrix
  $\Q$ that is a $n \times p$ orthonormal matrix (which forms an ONB
  for $C(\X)$) and $\mat{R}$ which is a $p
  \times p$ upper triangular matrix (i.e all elements below the
  diagonal are 0) where $\X = \Q \mat{R}$. The columns of $\Q$ are an ONB for
  the $C(\X)$. Show that $\P_\X$
  may be expressed as a function of $\Q$ alone.   Show that  the 
 the normal equations reduce to solving the triangular system $\mat{R} \b = \Z$ where $\Z = \Q^T \Y$.
 Because $\mat{R}$ is upper triangular, show that $\hat{\b}$ may be
 obtained be back-solving (and avoiding the matrix inverse of $\X^T\X$.
   
\item Any symmetric matrix $\A$ may be written via a Cholesky
  decomposition as $\A = \L \L^T$ where $\L$
  is lower triangular.   If $\Z = \X^T\Y$  show that we can solve two
  triangular systems $\L \L^T \b = \Z$ by solving for $\w$ using  $\L \w = \Z$ using a
  forward substitution and then for $\hat{\b}$ using $\L^T \b =
  \w$ avoiding any matrix inversion.

\item Use $\R$ to find $\Q$ and $\U$ for the matrices in problems 1.5.8 in
  Christensen. Does $\Q$ equal $\U$?   See help pages via {\tt
    help(qr)} and {\tt help(svd)} for function documentation.
\item Prove that the two projection matrices obtained by the SVD and
  the QR method are the same.  (Hint:  review Theorems in Christensen
  Appencicies about uniqueness of projections)

  \end{enumerate}
  Note: The Cholesky method is the fastest in terms of
  $O(n p^2 + p^3/3)$ floating point operations (flops), but is
  numerically unstable if the matrix is poorly conditioned.  R uses
  the QR method ($O(2 n p^2 - 2p^3/3)$ flops in the function {\tt lm.fit()}
  (which is the workhorse underneath the {\tt lm()} function.  Generalized QR
  algorithms can handle rank deficient case.  The SVD method is the
  most expensive $O(2 n p^2 + 11 p^3)$ but can handle the rank case.
  There are generalized Cholesky and QR methods for the rank deficient
  case.
\end{enumerate}
\end{document}

