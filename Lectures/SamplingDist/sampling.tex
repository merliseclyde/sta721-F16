\documentclass[handout]{beamer}
%\documentclass{beamer}
%\usepackage[dvips]{color}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{../macros}
\usepackage{verbatim}

\usetheme{Warsaw}
\title{Sampling Distributions}
\subtitle{Merlise Clyde}
\author{STA721 Linear Models}
\institute{Duke University}
\date{September 8, 2016}
\logo{duke.eps}

\begin{document}
\maketitle

\begin{frame}\frametitle{Outline}
Topics 
  \begin{itemize}
  \item Normal Theory
  \item Chi-squared Distributions
  \item Student $t$ Distributions
  \end{itemize}

\vspace{24pt}
Readings:  Christensen Apendix C, Chapter 1-2 
\end{frame}
%\section{Models}

\begin{frame}[fragile]
  \frametitle{Prostate Example}
  \begin{small}
\begin{verbatim}
> library(lasso2); data(Prostate)    # n = 97, 9 variables
> summary(lm(lpsa ~ ., data=Prostate))
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.669399   1.296381   0.516  0.60690    
lcavol       0.587023   0.087920   6.677 2.11e-09 ***
lweight      0.454461   0.170012   2.673  0.00896 ** 
age         -0.019637   0.011173  -1.758  0.08229 .  
lbph         0.107054   0.058449   1.832  0.07040 .  
svi          0.766156   0.244309   3.136  0.00233 ** 
lcp         -0.105474   0.091013  -1.159  0.24964    
gleason      0.045136   0.157464   0.287  0.77506    
pgg45        0.004525   0.004421   1.024  0.30885    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.7084 on 88 degrees of freedom
Multiple R-squared:  0.6548,	Adjusted R-squared:  0.6234 
F-statistic: 20.86 on 8 and 88 DF,  p-value: < 2.2e-16
\end{verbatim}
    
\end{small}

\end{frame}

\begin{frame}
  \frametitle{Summary of Distributions}

Models:  Full  $\Y = \X\b + \eps$ 

Assume $\X$ is full rank with the first column of ones $\one_n$ and $p$ additional predictors $r(\X) = p +1$

   $$\hat{\b} \mid \sigma^2 \sim \N(\b, \sigma^2 (\X^T\X)^{-1})$$
   $$\frac{\SSE }{\sigma^2} \sim \chi^2_{n - r(\X)}$$
   $$\frac{\hat{\beta}_j - \beta_j}{\SE(\hat{\beta}_j)} \sim t_{n - r(\X)}$$
where  $\SE(\hat{\beta})$  is the square root of the  $j$th diagonal element of $\hat{\sigma}^2 (\X^T\X)^{-1}$   and $\hat{\sigma}^2$ is the unbiased estimate of $\sigma^2$
\end{frame}

\section{Sampling Distribution}

\begin{frame} \frametitle{General Case}
$\W = \mub + \A \Z$  with $\Z \in \bbR^d$ and $\A$ is $n \times d$ \pause
\begin{itemize}
\item $\E[\W] = \mu$ \pause
\item $\Cov(\W) = \A \A^T \ge 0$ \pause
\item $\W \sim \N(\mub, \Sigmab)$ where $\Sigmab = \A \A^T$
\end{itemize}
  If $\Sigmab$ is singular then there is no density (on $\bbR^n$), but claim that
  $\W$ still has a multivariate normal distribution!  \pause

 \begin{definition}
  $\W \in \bbR^n$ has a  multivariate normal distribution $\N(\mub,
  \Sigmab)$ if for any $\v \in \bbR^n$ $\v^T\Y$ has a normal
  distribution with mean $\v^T\mub$ and variance $\v^T\Sigmab \v$
  \end{definition} \pause

see Lessons in Normal Theory in Sakai for videos using Characteristic functions
\end{frame}

\begin{frame} \frametitle{Linear Transformations are Normal}

If $\Y \sim \N_n(\mub, \Sigmab)$ then for $\A$ $m \times n$

$$\A \Y \sim \N_m(\A \mub, \A \Sigmab \A^T)$$


$\A \Sigmab \A^T$ does not have to be positive definite!
  


\end{frame}
\begin{frame}
  \frametitle{Equal in Distribution}
  Multiple ways to define the same normal: \pause

  \begin{itemize}
  \item 
$\Z_1 \sim \N(\zero, \I_n)$, $\Z_1 \in \bbR^n$  and take $\A$ $d
\times n$ \pause
\item $\Z_2 \sim \N(\zero, \I_p)$,  $\Z_2 \in \bbR^p$  and take $\B$ $d
\times p$ \pause
\item Define $\Y = \mub + \A \Z_1$ \pause
\item Define $\W = \mub + \B \Z_2$ \pause
  \end{itemize}
  \begin{theorem}
    If  $\Y = \mub + \A \Z_1$ and $\W = \mub + \B \Z_2$ then $\Y
    \eqindis \W$ if and only if $\A \A^T = \B \B^T = \Sigmab$
  \end{theorem}
\end{frame}

\frame{ \frametitle{Zero Correlation and Independence}
 \begin{theorem}
For a random vector $\Y \sim \N(\mub, \Sigmab) $ partitioned as
$$
\Y = \left[
  \begin{array}{c}
\Y_1  \\ \Y_2 \end{array} \right]  \sim \N\left( \left[
  \begin{array}{c} \mub_1  \\ \mub_2 \end{array} \right],
  \left[ \begin{array}{cc}
\Sigmab_{11} &  \Sigmab_{12}  \\ 
\Sigmab_{21} & \Sigmab_{22} \end{array} \right]
 \right)    
 $$  \pause
then $\Cov(\Y_1, \Y_2) = \Sigmab_{12} = \Sigmab_{21}^T = \zero$  if and
only if $\Y_1$ and $\Y_2$ are independent.
  \end{theorem}
}

\frame { \frametitle{Independence Implies Zero Covariance}
\begin{proof}
$$ \Cov(\Y_1, \Y_2) = \E[ (\Y_1 - \mub_1)(\Y_2 - \mub_2)^T]$$ \pause
  If $\Y_1$ and $\Y_2$ are independent \pause 
$$\E[ (\Y_1 - \mub_1)(\Y_2 - \mub_2)^T] = \E[ (\Y_1 - \mub_1) \E(\Y_2 -
\mub_2)^T] = \zero \zero^T = \zero $$ \pause
 
therefore $\Sigmab_{12} = \zero$

\end{proof}

}

\frame { \frametitle{ Zero Covariance Implies  Independence}
  Assume $\Sigmab_{12} = \zero$
\begin{block}{Proof}
\begin{itemize}

\item  Choose an $$ 
  \A = \left[
  \begin{array}{ll}
    \A_1 & \zero \\
    \zero & \A_2 
  \end{array}
\right]  $$
 such that $\A_1 \A_1^T = \Sigmab_{11}$, $\A_2 \A_2^T = \Sigmab_{22}$
 \pause
 \item Partition  $$
\Z = \left[
  \begin{array}{c}
    \Z_1 \\ \Z_2
  \end{array}
\right] \sim \N\left(
\left[
  \begin{array}{c}
    \zero_1 \\ \zero_2
  \end{array}
\right],
\left[
  \begin{array}{ll}
    \I_1 &\zero \\
\zero & \I_2
  \end{array}
\right]  
 \right)  \text{ and } \mub = \left[
  \begin{array}{c}
    \mub_1 \\ \mub_2
  \end{array}
\right] $$ \pause
\item then 
       $\Y \eqindis \A \Z + \mub \sim  \N(\mub, \Sigmab)$ 
\end{itemize}
  \end{block}  
}
\frame {\frametitle{Continued}
  \begin{proof}
    \begin{itemize}
    \item $$
\left[
  \begin{array}{c}
    \Y_1 \\ \Y_2
  \end{array}
\right]  \eqindis \left[
  \begin{array}{c}
    \A_1\Z_1 + \mub_1 \\ \A_2\Z_2 +\mub_2
  \end{array}
\right] 
$$ \pause
\item But $\Z_1$ and $\Z_2$ are independent \pause
\item Functions of $\Z_1$ and $\Z_2$ are independent \pause
\item Therefore $\Y_1$ and $\Y_2$ are independent  \pause
    \end{itemize}
  \end{proof}
For Multivariate Normal Zero Covariance implies independence

}
\frame { \frametitle{ Another Useful Result}
  \begin{corollary}
    If $\Y \sim \N( \mub, \sigma^2 \I_n) $ and $\A \B^T = \zero$
then $\A \Y$ and $\B \Y$ are independent.
  \end{corollary} \pause
  \begin{proof}
 \begin{itemize}
    \item $$
\left[
  \begin{array}{c}
    \W_1 \\ \W_2
  \end{array}
\right]  = \left[
  \begin{array}{c}
    \A \\ \B 
  \end{array}
\right]  \Y =  \left[
  \begin{array}{c}
    \A \Y \\ \B  \Y
  \end{array}
\right] 
$$ \pause
\item $\Cov(\W_1, \W_2) = \Cov(\A \Y, \B \Y) = \sigma^2 \A \B^T$
  \pause
\item $\A \Y $ and $\B \Y$ are independent if $\A \B^T = \zero$
\end{itemize}    
  \end{proof}

}



\begin{frame}
  \frametitle{Sampling Distribution of $\b$}
If  $\Y \sim \N(\X\b, \sigma^2 \I_n)$

Then $\hat{\b} \sim \N(\b, \sigma^2 (\X^T\X)^{-1})$
\vfill

\end{frame}


\begin{frame} \frametitle{Unknown $\sigma^2$}

  \begin{align*}
    \hat{\beta}_j \mid \beta_j, \sigma^2 & \sim \N(\beta, { \sigma^2}
                                           [(\X^T\X)^{-1}]_{jj})
  \end{align*}  \pause

What happens if we substitute $\hat{\sigma}^2 = \e^t\e/(n-r(\X))$ in the above? \pause

$$
\frac{(\hat{\beta}_j - \beta_j)/ \sigma \sqrt{ [(\X^T\X)^{-1}]_{jj}}}
{\sqrt{\e^T\e/ (\sigma^2 (n - r(\X))} } \eqindis
\frac{N(0,1)}{\sqrt{\chi_{n-r(\X)}^2/(n - r(\X)}} \sim t(n - r(\X), 0 ,1)
$$

Need to show that $\e^T\e /\sigma^2$ has a $\chi^2$ distribution and
is independent of the numerator!
\end{frame}

\begin{frame}
  \frametitle{Central Student $t$ Distribution}
  \begin{definition}
    Let $Z \sim \N(0, 1)$ and $S \sim \chi^2_p$ with $Z$ and $S$
    independent, \pause then
 $$ W = \frac{Z} {\sqrt{S/p}}$$
has a (central) Student $t$ distribution with $p$ degrees of freedom
  \end{definition}
\pause
 
See Casella \& Berger or DeGroot \& Schervish for derivation - nice change of variables and marginalization problem!
\end{frame}






\begin{frame}
  \frametitle{Chi-Squared Distribution}
  \begin{Definition}
    If $Z \sim \N(0,1)$ then $Z^2 \sim \chi^2_1$ (A Chi-squared
    distribution with one degree of freedom) \pause
    \begin{itemize}
    \item Density
$$
f(x) = \frac{1}{\Gamma(1/2)} (1/2)^{-1/2} x^{1/2 - 1} e^{-x/2} \qquad x
> 0
$$ \pause
  \item  Characteristic Function
$$
\E[e^{itZ^2}] = \varphi(t) = (1 - 2 i t)^{-1/2}
$$
  \end{itemize}

\end{Definition}
\end{frame}

\begin{frame}
  \frametitle{Chi-Squared Distribution with $p$ Degrees of Freedom}
If $Z_j \simiid \N(0,1)$ $j = 1, \ldots p$ then $X \equiv \Z^T\Z = \sum_j^p
Z_j^2 \sim \chi^2_p$  \pause

\begin{block}{Characteristic Function}
\begin{eqnarray*}
  \varphi_{X}(t) & = & \E[e^{it \sum_j^p
Z_j^2}] \pause \\
& = & \prod_{j=1}^p \E[e^{it Z_j^2 }] \pause \\
& = &  \prod_{j=1}^p (1 - 2 i t)^{-1/2} \pause \\
& = & (1 - 2 i t)^{-p/2} \pause
\end{eqnarray*}
A  Gamma distribution with shape $p/2$ and rate $1/2$, $G(p/2, 1/2)$ 

$$
f(x) = \frac{1}{\Gamma(p/2)} (1/2)^{-p/2} x^{p/2 - 1} e^{-x/2} \qquad x
> 0
$$
 \end{block}
\end{frame}
\begin{frame}
  \frametitle{Quadratic Forms}
  \begin{theorem}
  Let  $\Y \sim  \N(\mub, \sigma^2 \I_n)$ with $\mub \in C(\X)$ then if $\Q$ is
  a rank $k$ orthogonal  projection on to $C(\X)^{\perp}$,
$(\Y^T \Q \Y)/\sigma^2 \sim \chi^2_k$
  \end{theorem}
  \begin{proof}
    For an orthogonal projection  $\Q = \U \Lambdab \U^T 
    = \U_k \U_k^T$ where $C(\Q) = C(\U_k)$ and $\U_k^T\U_k = \I_k$
    (Spectral Theorem) \pause
    \begin{eqnarray*}
\Y^T\Q \Y  &=& \Y^T\U_k \U_k^T \Y      \pause \\
\Z & =  &\U_k^T \Y/ \sigma  \sim \N(\U_k^T\mub, \U_k^T\U_k) \pause \\
\Z & \sim & \N(\zero, \I_k) \pause \\
\Z^T\Z & \sim & \chi^2_k \pause 
    \end{eqnarray*}
Since $U^T\Y /\sigma \eqindis \Z$, $\frac{\Y^T\Q \Y}{\sigma^2} \sim
\chi^2_k$


  \end{proof}
\end{frame}

\begin{frame} \frametitle{Residual Sum of Squares Example}
 
\begin{block}{Sum of Squares Error  (SSE)}
Let $\Y \sim \N(\mub, \sigma^2 \I_n)$ with $\mub \in C(\X)$.


Because $\mub \in C(\X)$, $\I - \P_{\X}$ is a projection on
$C(\X)^{\perp}$ \pause $$\frac{\e^T\e}{\sigma^2} = \Y^T\frac{(\I_n - \P_\X)}\sigma^2 \Y  \sim
 \chi^2_{n - r(\X)}$$ 
\end{block}
\end{frame}

\begin{frame}
  \frametitle{ Estimated Coefficients and Residuals are Independent}
If  $\Y \sim \N(\X\b, \sigma^2 \I_n)$

Then $\Cov(\hat{\b}, \e) = \zero$ which implies independence 
\vfill

Functions of independent random variables are independent
(show characteristic functions or densities factor)
\end{frame}

\begin{frame}\frametitle{Putting it all together}
$\hat{\b} \sim \N(\b, \sigma^2 (\X^T\X)^{-1})$
  \begin{itemize}
  \item $(\hat{\beta}_j - \beta_j)/ \sigma [(\X^T\X)^{-1}]_{jj} \sim
    \N(0,1)$
\item $\e^T\e/ \sigma^2 \sim \chi^2_{n - r(\X)}$
\item $\hat{\beta}$ and $\e$ are independent
$$ \frac{(\hat{\beta}_j - \beta_j)/ \sigma [(\X^T\X)^{-1}]_{jj}}
{\sqrt{\e^T\e/ ( \sigma^2 (n - r(\X)))}} \sim t(n - r(\X), 0, 1)$$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Inference}
\vspace{-.5in}

  \begin{itemize}
  \item 
   95\% Confidence interval:  $\hat{\beta}_j \pm t_{\alpha/2}
   \SE(\hat{\beta}_j)$  \pause use {\tt qt(a, df)} for $t_a$ quantile
\item derive from pivotal quantity $t = (\hat{\beta}_j -
  \beta_j)/\SE(\hat{\beta}_j)$ where 
 $P(t \in (t_{\alpha/2}, t_{1 - \alpha/2}))  = 1 -\alpha$
  \end{itemize}

\vfill

\end{frame}
\begin{frame}\frametitle{Prostate Example}
{\tt xtable(confint(prostate.lm))}  from library(MASS) and library(xtable)
  \begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & 2.5 \% & 97.5 \% \\ 
  \hline
(Intercept) & -1.91 & 3.25 \\ 
  lcavol & 0.41 & 0.76 \\ 
  lweight & 0.12 & 0.79 \\ 
  age & -0.04 & 0.00 \\ 
  lbph & -0.01 & 0.22 \\ 
  svi & 0.28 & 1.25 \\ 
  lcp & -0.29 & 0.08 \\ 
  gleason & -0.27 & 0.36 \\ 
  pgg45 & -0.00 & 0.01 \\ 
   \hline
\end{tabular}
\end{table}
\end{frame}
\begin{frame} \frametitle{interpretation}
  \begin{itemize}
  \item 
  For a ``1'' unit increase in $\X_j$, expect $\Y$ to increase by $\hat{\beta}_j \pm t_{\alpha/2}
   \SE(\hat{\beta}_j)$ 
\item for log transforms
$$\Y = \exp(\X\b + \eps) = \prod \exp(\X_j \beta_j) \exp(\eps)$$
\item if $\X = \log(\W_j)$ then look at 2-fold or \%
  increases in $\W$ to look at multiplicative increase in median of $\Y$
\item if{\tt  cavol} increases by 10\%  then we expect {\tt PSA} to increase
  by $1.10^{(CI)}$  = $( 1.0398 \%, 1.0751 \%)$ or by 3.98 to 7.51 percent
  \end{itemize}
For a 10\% increase in cancer volume, we are 95\% confident  that the PSA levels
will increase by approximately 4 to 7.5 percent.
\end{frame}
\begin{frame}\frametitle{Derivation}
  
\end{frame}
\end{document}




